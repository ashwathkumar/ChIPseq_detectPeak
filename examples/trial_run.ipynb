{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial run on small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "validation_dir = 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150 #Image dimensions\n",
    "train_samples = 2000 #number of training examples. 1000 in each group. \n",
    "validation_samples = 800 # number of validation examples. 400 in each group. \n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16 with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_from_vgg():\n",
    "    vgg_model = applications.VGG16(include_top=False, weights='imagenet') #VGG16 load the model without top layer. \n",
    "    #Data generators so that I can extract the features without the top layer using weights from VGG16\n",
    "    datagenerator = ImageDataGenerator(rescale=1. / 255)\n",
    "    train_generator = datagenerator.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    validation_generator = datagenerator.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    #Predict features\n",
    "    features_train = vgg_model.predict_generator(\n",
    "        train_generator, \n",
    "        train_samples // batch_size)\n",
    "    features_validation = vgg_model.predict_generator(\n",
    "        validation_generator, \n",
    "        validation_samples // batch_size)\n",
    "    #since we have equal number of samples in both categories and we used shuffle = FALSE in our datagenerator above. Thus, we can just make the labels in this case.  \n",
    "    return features_train,features_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make top layer and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_last_layer(features_train,train_labels,features_validation,validation_labels):\n",
    "    #Make the top layer. \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=features_train.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))#signmoid since we have only two classes in our case - Peak or no peak. \n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy']) #compile with rmsprop. We can also use Adam - seems to be doing about the same performance. \n",
    "    #Train the top layer with training and validation data. \n",
    "    model.fit(features_train, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(features_validation, validation_labels))\n",
    "    model.save_weights('vgg16_transfer_top_layer.h5')\n",
    "    model.save('vgg16_transfer_top_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run functions to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "features_train,features_validation = save_features_from_vgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since shuffle was off and there was equal number of both classes we can just hand make the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([0] * int(train_samples / 2) + [1] * int(train_samples / 2))\n",
    "validation_labels = np.array([0] * int(validation_samples / 2) + [1] * int(validation_samples / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\Ashwath\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\Users\\Ashwath\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Users\\Ashwath\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0995 - acc: 0.9710 - val_loss: 0.0107 - val_acc: 0.9975\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0152 - acc: 0.9940 - val_loss: 0.0081 - val_acc: 0.9988\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.3670 - val_acc: 0.9387\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.9990 - val_loss: 0.0494 - val_acc: 0.9912\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0167 - val_acc: 0.9950\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0035 - acc: 0.9985 - val_loss: 0.0309 - val_acc: 0.9962\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0106 - acc: 0.9980 - val_loss: 0.0309 - val_acc: 0.9962\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0206 - val_acc: 0.9962\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 7.8044e-04 - acc: 0.9995 - val_loss: 0.1302 - val_acc: 0.9838\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0173 - val_acc: 0.9975\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 7.1485e-05 - acc: 1.0000 - val_loss: 0.0889 - val_acc: 0.9888\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 2.2339e-05 - acc: 1.0000 - val_loss: 0.0384 - val_acc: 0.9925\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 3.2613e-05 - acc: 1.0000 - val_loss: 0.0419 - val_acc: 0.9925\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 8.2767e-04 - acc: 0.9995 - val_loss: 0.0303 - val_acc: 0.9962\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 2.1055e-07 - acc: 1.0000 - val_loss: 0.0367 - val_acc: 0.9938\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 3.4448e-06 - acc: 1.0000 - val_loss: 0.0301 - val_acc: 0.9975\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0052 - acc: 0.9995 - val_loss: 0.0323 - val_acc: 0.9975\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 5.3898e-06 - acc: 1.0000 - val_loss: 0.0209 - val_acc: 0.9975\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.2032e-07 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9938\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1040e-07 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 0.9938\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.3203e-06 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9925\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.6317e-07 - acc: 1.0000 - val_loss: 0.0384 - val_acc: 0.9962\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 2.2622e-07 - acc: 1.0000 - val_loss: 0.0865 - val_acc: 0.9925\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 6.0964e-07 - acc: 1.0000 - val_loss: 0.0342 - val_acc: 0.9950\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 4.0494e-05 - acc: 1.0000 - val_loss: 0.0313 - val_acc: 0.9962\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.2800e-07 - acc: 1.0000 - val_loss: 0.0402 - val_acc: 0.9938\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.2407e-07 - acc: 1.0000 - val_loss: 0.0318 - val_acc: 0.9962\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.3597e-07 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 0.9938\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.2338e-07 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9962\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0052 - acc: 0.9995 - val_loss: 0.0758 - val_acc: 0.9925\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 6.4785e-05 - acc: 1.0000 - val_loss: 0.0601 - val_acc: 0.9925\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1628e-07 - acc: 1.0000 - val_loss: 0.0413 - val_acc: 0.9938\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0563 - val_acc: 0.9925\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 7.4013e-07 - acc: 1.0000 - val_loss: 0.1670 - val_acc: 0.9838\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 1.0914e-06 - acc: 1.0000 - val_loss: 0.0407 - val_acc: 0.9938\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.2573e-07 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9938\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 2.5242e-06 - acc: 1.0000 - val_loss: 0.0420 - val_acc: 0.9938\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.2072e-06 - acc: 1.0000 - val_loss: 0.0391 - val_acc: 0.9938\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 3.2406e-04 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9975\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.2140e-07 - acc: 1.0000 - val_loss: 0.0237 - val_acc: 0.9975\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 2.0419e-06 - acc: 1.0000 - val_loss: 0.0751 - val_acc: 0.9925\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1652e-07 - acc: 1.0000 - val_loss: 0.0367 - val_acc: 0.9950\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 7.5620e-07 - acc: 1.0000 - val_loss: 0.0795 - val_acc: 0.9925\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1100e-07 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 0.9925\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.0981e-07 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9925\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1367e-07 - acc: 1.0000 - val_loss: 0.0391 - val_acc: 0.9938\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.1055e-07 - acc: 1.0000 - val_loss: 0.0479 - val_acc: 0.9938\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.4059e-07 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 0.9975\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 4.1359e-07 - acc: 1.0000 - val_loss: 0.0739 - val_acc: 0.9925\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1079e-07 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9925\n"
     ]
    }
   ],
   "source": [
    "train_last_layer(features_train,train_labels,features_validation,validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image,model_file):\n",
    "    img = load_img(image,target_size=(150,150))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    vgg_model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    model = load_model(model_file)\n",
    "    features = vgg_model.predict(img)\n",
    "    predictVal = model.predict(features)\n",
    "    if predictVal[0][0] == 0:\n",
    "        print (\"It is not a peak.\")\n",
    "    else:\n",
    "        print (\"It is a peak.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a peak.\n"
     ]
    }
   ],
   "source": [
    "predict_image(\"test/peak.png\", \"vgg16_transfer_top_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
